{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eae03b7f-2fbc-4d0e-8391-3e82089a0612",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c752b9f5-9fb9-4c4e-bbc6-a1e056693b86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, Dropout, LayerNormalization, GlobalAveragePooling1D, Reshape, Add\n",
    "from tensorflow.keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be0440d2-080c-4090-b8ce-112302271fa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('netflix_single_user_balanced_pattern_v5.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac94b3c1-1236-45ee-9a39-3b089e98a0b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>mac_address</th>\n",
       "      <th>service_group</th>\n",
       "      <th>service_name</th>\n",
       "      <th>device_type</th>\n",
       "      <th>usage_minutes</th>\n",
       "      <th>usage_percentage</th>\n",
       "      <th>signal_strength</th>\n",
       "      <th>packet_loss_rate</th>\n",
       "      <th>latency</th>\n",
       "      <th>jitter_ms</th>\n",
       "      <th>traffic_spike</th>\n",
       "      <th>bandwidth_speed_per_sec_mbps</th>\n",
       "      <th>buffer_occupancy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user1</td>\n",
       "      <td>2023-01-01 00:00:00</td>\n",
       "      <td>02:00:00:3e:60:f7</td>\n",
       "      <td>Social Media</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>6</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>-46.89</td>\n",
       "      <td>0.9104</td>\n",
       "      <td>19.78</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0</td>\n",
       "      <td>6.15</td>\n",
       "      <td>0.4301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user1</td>\n",
       "      <td>2023-01-01 00:15:00</td>\n",
       "      <td>02:00:00:ea:f9:7c</td>\n",
       "      <td>Streaming</td>\n",
       "      <td>Prime Video</td>\n",
       "      <td>TV</td>\n",
       "      <td>26</td>\n",
       "      <td>43.333333</td>\n",
       "      <td>-45.01</td>\n",
       "      <td>0.2980</td>\n",
       "      <td>47.07</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1</td>\n",
       "      <td>10.49</td>\n",
       "      <td>0.3375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user            timestamp        mac_address service_group service_name  \\\n",
       "0  user1  2023-01-01 00:00:00  02:00:00:3e:60:f7  Social Media     LinkedIn   \n",
       "1  user1  2023-01-01 00:15:00  02:00:00:ea:f9:7c     Streaming  Prime Video   \n",
       "\n",
       "  device_type  usage_minutes  usage_percentage  signal_strength  \\\n",
       "0      Mobile              6         10.000000           -46.89   \n",
       "1          TV             26         43.333333           -45.01   \n",
       "\n",
       "   packet_loss_rate  latency  jitter_ms  traffic_spike  \\\n",
       "0            0.9104    19.78       7.30              0   \n",
       "1            0.2980    47.07       2.19              1   \n",
       "\n",
       "   bandwidth_speed_per_sec_mbps  buffer_occupancy  \n",
       "0                          6.15            0.4301  \n",
       "1                         10.49            0.3375  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ba2ebc3-3619-4dad-98d1-e8ab94c5baf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "def getWorkHours(hour, is_weekend):\n",
    "    return 9 <= hour <= 17 and not is_weekend\n",
    "\n",
    "def getTimeBlock(hour):\n",
    "    if hour >= 0 and hour < 12:\n",
    "        return \"morning\"\n",
    "    elif hour >= 12 and hour < 17:\n",
    "        return \"afternoon\"\n",
    "    elif hour >= 17 and hour < 24:\n",
    "        return \"night\"\n",
    "\n",
    "def getMeetingHours(hour, is_weekend):\n",
    "    return hour in [9, 10, 14, 15] and not is_weekend\n",
    "\n",
    "def getLunchHours(hour):\n",
    "    return hour in [12, 13]\n",
    "\n",
    "def getEveningHour(hour):\n",
    "    return 18 <= hour <= 22\n",
    "\n",
    "def formatServiceGroup(service):\n",
    "    if service == \"Gaming\":\n",
    "        return \"gaming\"\n",
    "    elif service == \"Social Media\":\n",
    "        return \"social_media\"\n",
    "    elif service == \"Software\":\n",
    "        return \"software\"\n",
    "    elif service == \"Shopping\":\n",
    "        return \"shopping\"\n",
    "    elif service == \"Streaming\":\n",
    "        return \"streaming\"\n",
    "    else:\n",
    "        return service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63701333-c585-4389-95cd-082383e69aba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Time-based features\n",
    "# df['month'] = df['timestamp'].dt.month\n",
    "# df['day_of_week'] = df['timestamp'].dt.dayofweek.astype('float32')\n",
    "# df['hour'] = df['timestamp'].dt.hour.astype('float32')\n",
    "# df['is_weekend'] = df['timestamp'].dt.dayofweek >= 5\n",
    "# df['is_business_hours'] = (df['timestamp'].dt.hour >= 9) & (df['timestamp'].dt.hour <= 17)\n",
    "# df['week_number'] = df['timestamp'].dt.isocalendar().week\n",
    "# # df['time_block'] = df['hour'].apply(getTimeBlock)\n",
    "# df['is_work_hour'] = df.apply(lambda row: getWorkHours(row['hour'], row['is_weekend']), axis=1)\n",
    "# df['is_meeting_hour'] = df.apply(lambda row: getMeetingHours(row['hour'], row['is_weekend']), axis=1)\n",
    "# df['is_lunch_hour'] = df['hour'].apply(getLunchHours)\n",
    "# df['is_evening'] = df['hour'].apply(getEveningHour)\n",
    "# df['service_group'] = df['service_group'].apply(formatServiceGroup)\n",
    "\n",
    "# # Ensure timestamp is sorted\n",
    "# df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "# df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# df_pivot = pd.pivot_table(df, values='usage_minutes', index=['timestamp', 'device_type', 'hour','day_of_week','is_weekend', 'is_work_hour', 'is_meeting_hour', 'is_lunch_hour', 'is_evening'], \n",
    "#                 columns='service_group', fill_value=0).reset_index()\n",
    "\n",
    "# dominant_service = df['service_group'].unique()\n",
    "# df_pivot['dominant_service'] = df[dominant_service].idxmax(axis=1)\n",
    "\n",
    "# df_pivot.rename(columns={'gaming': 'gaming_usage'}, inplace=True)\n",
    "# df_pivot.rename(columns={'shopping': 'shopping_usage'}, inplace=True)\n",
    "# df_pivot.rename(columns={'social_media': 'social_media_usage'}, inplace=True)\n",
    "# df_pivot.rename(columns={'streaming': 'streaming_usage'}, inplace=True)\n",
    "# df_pivot.rename(columns={'software': 'software_usage'}, inplace=True)\n",
    "\n",
    "# df_pivot.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cf02e13-daf4-4046-b7a3-fa1d5993c9ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service_group           timestamp device_type  hour  day_of_week  is_weekend  \\\n",
      "0             2023-01-01 00:00:00      Mobile   0.0          6.0        True   \n",
      "1             2023-01-01 00:15:00          TV   0.0          6.0        True   \n",
      "2             2023-01-01 00:30:00          TV   0.0          6.0        True   \n",
      "3             2023-01-01 00:45:00      Tablet   0.0          6.0        True   \n",
      "4             2023-01-01 01:00:00      Tablet   1.0          6.0        True   \n",
      "\n",
      "service_group  is_work_hour  is_meeting_hour  is_lunch_hour  is_evening  \\\n",
      "0                     False            False          False       False   \n",
      "1                     False            False          False       False   \n",
      "2                     False            False          False       False   \n",
      "3                     False            False          False       False   \n",
      "4                     False            False          False       False   \n",
      "\n",
      "service_group  gaming_usage  shopping_usage  social_media_usage  \\\n",
      "0                         0               0                   6   \n",
      "1                         0               0                   0   \n",
      "2                         0               0                   0   \n",
      "3                         0               0                   5   \n",
      "4                         0               0                  20   \n",
      "\n",
      "service_group  software_usage  streaming_usage dominant_service  \n",
      "0                           0                0     social_media  \n",
      "1                           0               26        streaming  \n",
      "2                           0               50        streaming  \n",
      "3                           0                0     social_media  \n",
      "4                           0                0     social_media  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Ensure the 'timestamp' column is in datetime format\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "\n",
    "# Check for and drop rows with invalid timestamps (optional)\n",
    "if df['timestamp'].isna().any():\n",
    "    print(\"Invalid timestamps detected. Dropping rows with invalid timestamps.\")\n",
    "    df = df.dropna(subset=['timestamp'])\n",
    "\n",
    "# Time-based features\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek.astype('float32')\n",
    "df['hour'] = df['timestamp'].dt.hour.astype('float32')\n",
    "df['is_weekend'] = df['timestamp'].dt.dayofweek >= 5\n",
    "df['is_business_hours'] = (df['timestamp'].dt.hour >= 9) & (df['timestamp'].dt.hour <= 17)\n",
    "df['week_number'] = df['timestamp'].dt.isocalendar().week\n",
    "\n",
    "# Apply time-based custom functions\n",
    "df['is_work_hour'] = df.apply(lambda row: getWorkHours(row['hour'], row['is_weekend']), axis=1)\n",
    "df['is_meeting_hour'] = df.apply(lambda row: getMeetingHours(row['hour'], row['is_weekend']), axis=1)\n",
    "df['is_lunch_hour'] = df['hour'].apply(getLunchHours)\n",
    "df['is_evening'] = df['hour'].apply(getEveningHour)\n",
    "\n",
    "# Ensure 'service_group' column exists before applying formatting\n",
    "if 'service_group' in df.columns:\n",
    "    df['service_group'] = df['service_group'].apply(formatServiceGroup)\n",
    "else:\n",
    "    raise ValueError(\"The 'service_group' column is missing from the DataFrame.\")\n",
    "\n",
    "# Ensure timestamp is sorted\n",
    "df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Create pivot table\n",
    "df_pivot = pd.pivot_table(\n",
    "    df,\n",
    "    values='usage_minutes',\n",
    "    index=['timestamp', 'device_type', 'hour', 'day_of_week', 'is_weekend', 'is_work_hour', 'is_meeting_hour', 'is_lunch_hour', 'is_evening'],\n",
    "    columns='service_group',\n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "# Ensure the service_group column is handled correctly for dominant_service\n",
    "if 'service_group' in df.columns:\n",
    "    unique_services = df['service_group'].unique()\n",
    "    df_pivot['dominant_service'] = df_pivot[unique_services].idxmax(axis=1)\n",
    "else:\n",
    "    raise ValueError(\"The 'service_group' column is missing or incorrectly processed.\")\n",
    "\n",
    "# Rename columns for clarity\n",
    "df_pivot.rename(columns={\n",
    "    'gaming': 'gaming_usage',\n",
    "    'shopping': 'shopping_usage',\n",
    "    'social_media': 'social_media_usage',\n",
    "    'streaming': 'streaming_usage',\n",
    "    'software': 'software_usage'\n",
    "}, inplace=True)\n",
    "\n",
    "# Display the first 5 rows\n",
    "print(df_pivot.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4c378e56-545e-4684-a4bc-f0143b7029af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>mac_address</th>\n",
       "      <th>service_group</th>\n",
       "      <th>service_name</th>\n",
       "      <th>device_type</th>\n",
       "      <th>usage_minutes</th>\n",
       "      <th>usage_percentage</th>\n",
       "      <th>signal_strength</th>\n",
       "      <th>packet_loss_rate</th>\n",
       "      <th>latency</th>\n",
       "      <th>jitter_ms</th>\n",
       "      <th>traffic_spike</th>\n",
       "      <th>bandwidth_speed_per_sec_mbps</th>\n",
       "      <th>buffer_occupancy</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hour</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_business_hours</th>\n",
       "      <th>week_number</th>\n",
       "      <th>is_work_hour</th>\n",
       "      <th>is_meeting_hour</th>\n",
       "      <th>is_lunch_hour</th>\n",
       "      <th>is_evening</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:00:00</th>\n",
       "      <td>user1</td>\n",
       "      <td>02:00:00:3e:60:f7</td>\n",
       "      <td>social_media</td>\n",
       "      <td>LinkedIn</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>6</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>-46.89</td>\n",
       "      <td>0.9104</td>\n",
       "      <td>19.78</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0</td>\n",
       "      <td>6.15</td>\n",
       "      <td>0.4301</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:15:00</th>\n",
       "      <td>user1</td>\n",
       "      <td>02:00:00:ea:f9:7c</td>\n",
       "      <td>streaming</td>\n",
       "      <td>Prime Video</td>\n",
       "      <td>TV</td>\n",
       "      <td>26</td>\n",
       "      <td>43.333333</td>\n",
       "      <td>-45.01</td>\n",
       "      <td>0.2980</td>\n",
       "      <td>47.07</td>\n",
       "      <td>2.19</td>\n",
       "      <td>1</td>\n",
       "      <td>10.49</td>\n",
       "      <td>0.3375</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:30:00</th>\n",
       "      <td>user1</td>\n",
       "      <td>02:00:00:9c:e3:5f</td>\n",
       "      <td>streaming</td>\n",
       "      <td>Netflix</td>\n",
       "      <td>TV</td>\n",
       "      <td>50</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>-41.17</td>\n",
       "      <td>0.7898</td>\n",
       "      <td>13.61</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.5050</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-01-01 00:45:00</th>\n",
       "      <td>user1</td>\n",
       "      <td>02:00:00:b2:f7:66</td>\n",
       "      <td>social_media</td>\n",
       "      <td>Instagram</td>\n",
       "      <td>Tablet</td>\n",
       "      <td>5</td>\n",
       "      <td>8.333333</td>\n",
       "      <td>-58.64</td>\n",
       "      <td>0.5335</td>\n",
       "      <td>38.34</td>\n",
       "      <td>3.95</td>\n",
       "      <td>1</td>\n",
       "      <td>12.19</td>\n",
       "      <td>0.3344</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>52</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      user        mac_address service_group service_name  \\\n",
       "timestamp                                                                  \n",
       "2023-01-01 00:00:00  user1  02:00:00:3e:60:f7  social_media     LinkedIn   \n",
       "2023-01-01 00:15:00  user1  02:00:00:ea:f9:7c     streaming  Prime Video   \n",
       "2023-01-01 00:30:00  user1  02:00:00:9c:e3:5f     streaming      Netflix   \n",
       "2023-01-01 00:45:00  user1  02:00:00:b2:f7:66  social_media    Instagram   \n",
       "\n",
       "                    device_type  usage_minutes  usage_percentage  \\\n",
       "timestamp                                                          \n",
       "2023-01-01 00:00:00      Mobile              6         10.000000   \n",
       "2023-01-01 00:15:00          TV             26         43.333333   \n",
       "2023-01-01 00:30:00          TV             50         83.333333   \n",
       "2023-01-01 00:45:00      Tablet              5          8.333333   \n",
       "\n",
       "                     signal_strength  packet_loss_rate  latency  jitter_ms  \\\n",
       "timestamp                                                                    \n",
       "2023-01-01 00:00:00           -46.89            0.9104    19.78       7.30   \n",
       "2023-01-01 00:15:00           -45.01            0.2980    47.07       2.19   \n",
       "2023-01-01 00:30:00           -41.17            0.7898    13.61       0.58   \n",
       "2023-01-01 00:45:00           -58.64            0.5335    38.34       3.95   \n",
       "\n",
       "                     traffic_spike  bandwidth_speed_per_sec_mbps  \\\n",
       "timestamp                                                          \n",
       "2023-01-01 00:00:00              0                          6.15   \n",
       "2023-01-01 00:15:00              1                         10.49   \n",
       "2023-01-01 00:30:00              0                          8.75   \n",
       "2023-01-01 00:45:00              1                         12.19   \n",
       "\n",
       "                     buffer_occupancy  month  day_of_week  hour  is_weekend  \\\n",
       "timestamp                                                                     \n",
       "2023-01-01 00:00:00            0.4301      1          6.0   0.0        True   \n",
       "2023-01-01 00:15:00            0.3375      1          6.0   0.0        True   \n",
       "2023-01-01 00:30:00            0.5050      1          6.0   0.0        True   \n",
       "2023-01-01 00:45:00            0.3344      1          6.0   0.0        True   \n",
       "\n",
       "                     is_business_hours  week_number  is_work_hour  \\\n",
       "timestamp                                                           \n",
       "2023-01-01 00:00:00              False           52         False   \n",
       "2023-01-01 00:15:00              False           52         False   \n",
       "2023-01-01 00:30:00              False           52         False   \n",
       "2023-01-01 00:45:00              False           52         False   \n",
       "\n",
       "                     is_meeting_hour  is_lunch_hour  is_evening  \n",
       "timestamp                                                        \n",
       "2023-01-01 00:00:00            False          False       False  \n",
       "2023-01-01 00:15:00            False          False       False  \n",
       "2023-01-01 00:30:00            False          False       False  \n",
       "2023-01-01 00:45:00            False          False       False  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4efad31b-22f4-432f-aeaa-316859120ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user                            67105\n",
       "mac_address                     67105\n",
       "service_group                   67105\n",
       "service_name                    67105\n",
       "device_type                     67105\n",
       "usage_minutes                   67105\n",
       "usage_percentage                67105\n",
       "signal_strength                 67105\n",
       "packet_loss_rate                67105\n",
       "latency                         67105\n",
       "jitter_ms                       67105\n",
       "traffic_spike                   67105\n",
       "bandwidth_speed_per_sec_mbps    67105\n",
       "buffer_occupancy                67105\n",
       "month                           67105\n",
       "day_of_week                     67105\n",
       "hour                            67105\n",
       "is_weekend                      67105\n",
       "is_business_hours               67105\n",
       "week_number                     67105\n",
       "is_work_hour                    67105\n",
       "is_meeting_hour                 67105\n",
       "is_lunch_hour                   67105\n",
       "is_evening                      67105\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ed9b009-8f07-4eb4-b7ef-ac3bcc826a49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# import joblib\n",
    "\n",
    "# def add_rolling_features(df):\n",
    "#     \"\"\"\n",
    "#     Add rolling statistics features based on usage minutes\n",
    "#     \"\"\"\n",
    "#     # Sort by timestamp to ensure correct rolling calculations\n",
    "#     df = df.sort_values('timestamp')\n",
    "\n",
    "#     # Number of 15-min intervals for different periods\n",
    "#     intervals = {\n",
    "#         'day': 96,  # 24 * 4 (15-min intervals)\n",
    "#         'week': 672,  # 7 * 24 * 4\n",
    "#         'two_weeks': 1344  # 14 * 24 * 4\n",
    "#     }\n",
    "\n",
    "#     # Calculate rolling statistics for each service\n",
    "#     services = [\"shopping\", \"gaming\", \n",
    "#                 \"social_media\", \"streaming\", \"software\"]\n",
    "\n",
    "#     print(\"services..... rolling calculation....\")\n",
    "#     for service in services:\n",
    "#         # print(f\"service ******************************** {service} ********************************\")\n",
    "#         usage_col = f'{service}_usage'\n",
    "#         # usage_col = f'{service}'\n",
    "\n",
    "#         # Rolling sums\n",
    "#         df[f'{service}_day_sum'] = df[usage_col].rolling(intervals['day'], min_periods=1).sum()\n",
    "#         df[f'{service}_week_sum'] = df[usage_col].rolling(intervals['week'], min_periods=1).sum()\n",
    "#         df[f'{service}_2week_sum'] = df[usage_col].rolling(intervals['two_weeks'], min_periods=1).sum()\n",
    "\n",
    "#         # print(f\"services_day_sum\", df[f'{service}_day_sum'])\n",
    "#         # print(f\"services_2week_sum\", df[f'{service}_2week_sum'])\n",
    "\n",
    "#         # Rolling means\n",
    "#         df[f'{service}_day_mean'] = df[usage_col].rolling(intervals['day'], min_periods=1).mean()\n",
    "#         df[f'{service}_week_mean'] = df[usage_col].rolling(intervals['week'], min_periods=1).mean()\n",
    "#         df[f'{service}_2week_mean'] = df[usage_col].rolling(intervals['two_weeks'], min_periods=1).mean()\n",
    "\n",
    "#         # Rolling max\n",
    "#         df[f'{service}_day_max'] = df[usage_col].rolling(intervals['day'], min_periods=1).max()\n",
    "#         df[f'{service}_week_max'] = df[usage_col].rolling(intervals['week'], min_periods=1).max()\n",
    "        \n",
    "        \n",
    "#         # Time since last peak usage (defined as usage > 75th percentile)\n",
    "#         peak_threshold = df[usage_col].quantile(0.75)\n",
    "#         df[f'{service}_since_peak'] = (df[usage_col] > peak_threshold).astype(int)\n",
    "#         df[f'{service}_intervals_since_peak'] = df[f'{service}_since_peak'].cumsum()\n",
    "\n",
    "#         # Calculate relative usage proportions\n",
    "#         total_usage = df[[f'{s}_usage' for s in services]].sum(axis=1)\n",
    "#         # total_usage = df[[f'{s}' for s in services]].sum(axis=1)\n",
    "#         for service in services:\n",
    "#             df[f'{service}_usage_ratio'] = df[f'{service}_usage'] / total_usage\n",
    "\n",
    "#         # Fill NaN values with 0\n",
    "#         df = df.fillna(0)\n",
    "#         # df.head(5)\n",
    "\n",
    "#         return df\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bbac992-6717-4a50-827f-05fa662acb11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def add_rolling_features(df):\n",
    "#     \"\"\"\n",
    "#     Add rolling statistics features based on usage minutes.\n",
    "#     \"\"\"\n",
    "#     # Sort by timestamp to ensure proper rolling calculations\n",
    "#     df = df.sort_values('timestamp')\n",
    "\n",
    "#     # Define rolling intervals\n",
    "#     intervals = {\n",
    "#         'day': 96,  # 24 * 4 (15-min intervals)\n",
    "#         'week': 672,  # 7 * 24 * 4\n",
    "#         'two_weeks': 1344  # 14 * 24 * 4\n",
    "#     }\n",
    "\n",
    "#     # List of services for rolling calculations\n",
    "#     services = [\"shopping\", \"gaming\", \"social_media\", \"streaming\", \"software\"]\n",
    "\n",
    "#     print(\"Adding rolling features for services...\")\n",
    "#     for service in services:\n",
    "#         usage_col = f'{service}_usage'\n",
    "\n",
    "#         # Skip if base usage column does not exist\n",
    "#         if usage_col not in df.columns:\n",
    "#             print(f\"Skipping {service}: '{usage_col}' not found in DataFrame.\")\n",
    "#             continue\n",
    "\n",
    "#         # Add rolling sum features\n",
    "#         df[f'{service}_day_sum'] = df[usage_col].rolling(intervals['day'], min_periods=1).sum()\n",
    "#         df[f'{service}_week_sum'] = df[usage_col].rolling(intervals['week'], min_periods=1).sum()\n",
    "#         df[f'{service}_2week_sum'] = df[usage_col].rolling(intervals['two_weeks'], min_periods=1).sum()\n",
    "\n",
    "#         # Add rolling mean features\n",
    "#         df[f'{service}_day_mean'] = df[usage_col].rolling(intervals['day'], min_periods=1).mean()\n",
    "#         df[f'{service}_week_mean'] = df[usage_col].rolling(intervals['week'], min_periods=1).mean()\n",
    "#         df[f'{service}_2week_mean'] = df[usage_col].rolling(intervals['two_weeks'], min_periods=1).mean()\n",
    "\n",
    "#         # Add rolling max features\n",
    "#         df[f'{service}_day_max'] = df[usage_col].rolling(intervals['day'], min_periods=1).max()\n",
    "#         df[f'{service}_week_max'] = df[usage_col].rolling(intervals['week'], min_periods=1).max()\n",
    "\n",
    "#         # Add time since last peak\n",
    "#         peak_threshold = df[usage_col].quantile(0.75)\n",
    "#         df[f'{service}_since_peak'] = (df[usage_col] > peak_threshold).astype(int)\n",
    "#         df[f'{service}_intervals_since_peak'] = df[f'{service}_since_peak'].cumsum()\n",
    "\n",
    "#     # Calculate relative usage proportions (outside the loop)\n",
    "#     print(\"Calculating usage ratios...\")\n",
    "#     total_usage = df[[f'{s}_usage' for s in services if f'{s}_usage' in df.columns]].sum(axis=1)\n",
    "#     for service in services:\n",
    "#         usage_col = f'{service}_usage'\n",
    "#         if usage_col in df.columns:\n",
    "#             df[f'{service}_usage_ratio'] = df[usage_col] / total_usage\n",
    "\n",
    "#     # Fill NaN values with 0\n",
    "#     df = df.fillna(0)\n",
    "\n",
    "#     print(\"Rolling features added for all services.\")\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc03083f-6fc8-4d29-b0e6-2a4575775e22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_rolling_features(df):\n",
    "    \"\"\"\n",
    "    Add rolling statistics features based on usage minutes.\n",
    "    \"\"\"\n",
    "    # Sort by timestamp to ensure proper rolling calculations\n",
    "    df = df.sort_values('timestamp')\n",
    "\n",
    "    # Define rolling intervals\n",
    "    intervals = {\n",
    "        'day': 96,  # 24 * 4 (15-min intervals)\n",
    "        'week': 672,  # 7 * 24 * 4\n",
    "        'two_weeks': 1344  # 14 * 24 * 4\n",
    "    }\n",
    "\n",
    "    # List of services for rolling calculations\n",
    "    services = [\"shopping\", \"gaming\", \"social_media\", \"streaming\", \"software\"]\n",
    "\n",
    "    print(\"Adding rolling features for services...\")\n",
    "    for service in services:\n",
    "        usage_col = f'{service}_usage'\n",
    "\n",
    "        # Skip if base usage column does not exist\n",
    "        if usage_col not in df.columns:\n",
    "            print(f\"Skipping {service}: '{usage_col}' not found in DataFrame.\")\n",
    "            continue\n",
    "\n",
    "        # Add rolling sum features\n",
    "        df[f'{service}_day_sum'] = df[usage_col].rolling(intervals['day'], min_periods=1).sum()\n",
    "        df[f'{service}_week_sum'] = df[usage_col].rolling(intervals['week'], min_periods=1).sum()\n",
    "        df[f'{service}_2week_sum'] = df[usage_col].rolling(intervals['two_weeks'], min_periods=1).sum()\n",
    "\n",
    "        # Add rolling mean features\n",
    "        df[f'{service}_day_mean'] = df[usage_col].rolling(intervals['day'], min_periods=1).mean()\n",
    "        df[f'{service}_week_mean'] = df[usage_col].rolling(intervals['week'], min_periods=1).mean()\n",
    "        df[f'{service}_2week_mean'] = df[usage_col].rolling(intervals['two_weeks'], min_periods=1).mean()\n",
    "\n",
    "        # Add rolling max features\n",
    "        df[f'{service}_day_max'] = df[usage_col].rolling(intervals['day'], min_periods=1).max()\n",
    "        df[f'{service}_week_max'] = df[usage_col].rolling(intervals['week'], min_periods=1).max()\n",
    "\n",
    "        # Add time since last peak\n",
    "        peak_threshold = df[usage_col].quantile(0.75)\n",
    "        df[f'{service}_since_peak'] = (df[usage_col] > peak_threshold).astype(int)\n",
    "        df[f'{service}_intervals_since_peak'] = df[f'{service}_since_peak'].cumsum()\n",
    "\n",
    "    # Calculate relative usage proportions\n",
    "    print(\"Calculating usage ratios...\")\n",
    "    total_usage = df[[f'{s}_usage' for s in services]].sum(axis=1)\n",
    "    for service in services:\n",
    "        df[f'{service}_usage_ratio'] = df[f'{service}_usage'] / total_usage\n",
    "\n",
    "    # Fill NaN values with 0\n",
    "    df = df.fillna(0)\n",
    "\n",
    "    print(\"Rolling features added for all services.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5bf9cbe7-dedf-400b-824c-ad0f3dbbed80",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding rolling features for services...\n",
      "Calculating usage ratios...\n",
      "Rolling features added for all services.\n",
      "Columns in df_pivot after rolling features: Index(['timestamp', 'device_type', 'hour', 'day_of_week', 'is_weekend',\n",
      "       'is_work_hour', 'is_meeting_hour', 'is_lunch_hour', 'is_evening',\n",
      "       'gaming_usage', 'shopping_usage', 'social_media_usage',\n",
      "       'software_usage', 'streaming_usage', 'dominant_service',\n",
      "       'shopping_day_sum', 'shopping_week_sum', 'shopping_2week_sum',\n",
      "       'shopping_day_mean', 'shopping_week_mean', 'shopping_2week_mean',\n",
      "       'shopping_day_max', 'shopping_week_max', 'shopping_since_peak',\n",
      "       'shopping_intervals_since_peak', 'gaming_day_sum', 'gaming_week_sum',\n",
      "       'gaming_2week_sum', 'gaming_day_mean', 'gaming_week_mean',\n",
      "       'gaming_2week_mean', 'gaming_day_max', 'gaming_week_max',\n",
      "       'gaming_since_peak', 'gaming_intervals_since_peak',\n",
      "       'social_media_day_sum', 'social_media_week_sum',\n",
      "       'social_media_2week_sum', 'social_media_day_mean',\n",
      "       'social_media_week_mean', 'social_media_2week_mean',\n",
      "       'social_media_day_max', 'social_media_week_max',\n",
      "       'social_media_since_peak', 'social_media_intervals_since_peak',\n",
      "       'streaming_day_sum', 'streaming_week_sum', 'streaming_2week_sum',\n",
      "       'streaming_day_mean', 'streaming_week_mean', 'streaming_2week_mean',\n",
      "       'streaming_day_max', 'streaming_week_max', 'streaming_since_peak',\n",
      "       'streaming_intervals_since_peak', 'software_day_sum',\n",
      "       'software_week_sum', 'software_2week_sum', 'software_day_mean',\n",
      "       'software_week_mean', 'software_2week_mean', 'software_day_max',\n",
      "       'software_week_max', 'software_since_peak',\n",
      "       'software_intervals_since_peak', 'shopping_usage_ratio',\n",
      "       'gaming_usage_ratio', 'social_media_usage_ratio',\n",
      "       'streaming_usage_ratio', 'software_usage_ratio'],\n",
      "      dtype='object', name='service_group')\n"
     ]
    }
   ],
   "source": [
    "df_pivot = add_rolling_features(df_pivot)\n",
    "print(\"Columns in df_pivot after rolling features:\", df_pivot.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b249b2d-4a68-4cf2-8941-2f78e1287add",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_sequences(df, seq_length=672, pred_length=672):\n",
    "    \"\"\"\n",
    "    Prepare sequences with enhanced features\n",
    "    \"\"\"\n",
    "    # Add cyclical time features\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "\n",
    "    # Define time-based features to use\n",
    "    base_features = [\n",
    "        'hour_sin', 'hour_cos', 'day_sin', 'day_cos',\n",
    "        'is_weekend', 'is_work_hour', 'is_meeting_hour', 'is_lunch_hour',\n",
    "        'is_evening'\n",
    "    ]\n",
    "\n",
    "    usage_features = [\n",
    "        'software_usage', 'streaming_usage',\n",
    "        'social_media_usage', 'shopping_usage', 'gaming_usage',\n",
    "        'software_day_sum', 'software_week_sum', 'software_2week_sum',\n",
    "        'streaming_day_sum', 'streaming_week_sum', 'streaming_2week_sum',\n",
    "        'social_media_day_sum', 'social_media_week_sum', 'social_media_2week_sum',\n",
    "        'gaming_day_sum', 'gaming_week_sum', 'gaming_2week_sum',\n",
    "        'shopping_day_sum', 'shopping_week_sum', 'shopping_2week_sum',\n",
    "        'software_usage_ratio', 'streaming_usage_ratio', 'social_media_usage_ratio',\n",
    "        'gaming_usage_ratio', 'shopping_usage_ratio',\n",
    "        'software_day_mean', 'streaming_day_mean', 'social_media_day_mean',\n",
    "        'gaming_day_mean', 'shopping_day_mean',\n",
    "        'software_intervals_since_peak', 'streaming_intervals_since_peak',\n",
    "        'social_media_intervals_since_peak', 'gaming_intervals_since_peak',\n",
    "        'shopping_intervals_since_peak'\n",
    "    ]\n",
    "\n",
    "    features = base_features + usage_features\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    df[features] = scaler.fit_transform(df[features])\n",
    "\n",
    "    # Encode services\n",
    "    le = LabelEncoder()\n",
    "    df['service_encoded'] = le.fit_transform(df['dominant_service'])\n",
    "    num_classes = len(le.classes_)\n",
    "\n",
    "    # Save Preprocessors\n",
    "    joblib.dump(scaler, 'service_scaler.pkl')\n",
    "    joblib.dump(le, 'label_encoders.pkl')\n",
    "\n",
    "    X, y = [], []\n",
    "    step = seq_length\n",
    "\n",
    "    for i in range(0, len(df) - seq_length - pred_length + 1, step):\n",
    "        X.append(df[features].iloc[i:i + seq_length].values)\n",
    "        y_seq = df['service_encoded'].iloc[i + seq_length:i + seq_length + pred_length].values\n",
    "        y.append(tf.keras.utils.to_categorical(y_seq, num_classes=num_classes))\n",
    "\n",
    "    return np.array(X), np.array(y), le, scaler, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95eed37c-ac13-4406-83ca-3439b9c8bd7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def prepare_sequences(df, seq_length=672, pred_length=672):\n",
    "#     features = [\n",
    "#         # Ensure these features are present\n",
    "#         'software_usage', 'streaming_usage', 'social_media_usage', 'gaming_usage', 'shopping_usage',\n",
    "#         'software_day_sum', 'software_week_sum', 'software_2week_sum',\n",
    "#         'streaming_day_sum', 'streaming_week_sum', 'streaming_2week_sum',\n",
    "#         'social_media_day_sum', 'social_media_week_sum', 'social_media_2week_sum',\n",
    "#         'gaming_day_sum', 'gaming_week_sum', 'gaming_2week_sum',\n",
    "#         'software_day_mean', 'streaming_day_mean', 'social_media_day_mean', 'gaming_day_mean',\n",
    "#         'software_intervals_since_peak', 'streaming_intervals_since_peak',\n",
    "#         'social_media_intervals_since_peak', 'gaming_intervals_since_peak',\n",
    "#     ]\n",
    "    \n",
    "#     # Verify all features are in DataFrame\n",
    "#     missing_features = [f for f in features if f not in df.columns]\n",
    "#     if missing_features:\n",
    "#         raise KeyError(f\"The following features are missing in the DataFrame: {missing_features}\")\n",
    "    \n",
    "#     # Proceed with scaling\n",
    "#     scaler = StandardScaler()\n",
    "#     df[features] = scaler.fit_transform(df[features])\n",
    "    \n",
    "    \n",
    "#     # Encode services\n",
    "#     le = LabelEncoder()\n",
    "#     df['service_encoded'] = le.fit_transform(df['dominant_service'])\n",
    "#     num_classes = len(le.classes_)\n",
    "\n",
    "#     # Save Preprocessors\n",
    "#     joblib.dump(scaler, 'service_scaler.pkl')\n",
    "#     joblib.dump(le, 'label_encoders.pkl')\n",
    "\n",
    "#     X, y = [], []\n",
    "#     step = seq_length\n",
    "\n",
    "#     for i in range(0, len(df) - seq_length - pred_length + 1, step):\n",
    "#         X.append(df[features].iloc[i:i + seq_length].values)\n",
    "#         y_seq = df['service_encoded'].iloc[i + seq_length:i + seq_length + pred_length].values\n",
    "#         y.append(tf.keras.utils.to_categorical(y_seq, num_classes=num_classes))\n",
    "\n",
    "#     return np.array(X), np.array(y), le, scaler, features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e4800362-df11-4286-8858-993f34167b28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Custom attention layer compatible with TFLite\n",
    "# class TFLiteCompatibleAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "#     def __init__(self, num_heads, key_dim, dropout=0.0, **kwargs):\n",
    "#         super().__init__(**kwargs)\n",
    "#         self.num_heads = num_heads\n",
    "#         self.key_dim = key_dim\n",
    "#         self.dropout_rate = dropout\n",
    "\n",
    "#         # Initialize dense layers for Q, K, V\n",
    "#         self.query_dense = Dense(num_heads * key_dim)\n",
    "#         self.key_dense = Dense(num_heads * key_dim)\n",
    "#         self.value_dense = Dense(num_heads * key_dim)\n",
    "#         self.combine_heads = Dense(key_dim * num_heads)\n",
    "\n",
    "#     def call(self, inputs):\n",
    "#         # Reshape inputs\n",
    "#         batch_size = tf.shape(inputs)[0]\n",
    "#         seq_len = tf.shape(inputs)[1]\n",
    "\n",
    "#         # Linear transformations\n",
    "#         query = self.query_dense(inputs)\n",
    "#         key = self.key_dense(inputs)\n",
    "#         value = self.value_dense(inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a55d0d9-f13e-4b8c-afde-fc307a505fe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Reshape to (batch_size, seq_len, num_heads, key_dim)\n",
    "# def reshape_for_attention(x):\n",
    "#     return tf.reshape([x, batch_size, seq_len, self.num_heads, self.key_dim])\n",
    "\n",
    "# # # Assume query, key, value, batch_size, seq_len, num_heads, key_dim are already defined\n",
    "# # batch_size = tf.shape(query)[0]\n",
    "# # seq_len = tf.shape(query)[1]\n",
    "\n",
    "# # Reshape query, key, and value\n",
    "# query = reshape_for_attention(query, batch_size, seq_len, self.num_heads, self.key_dim)\n",
    "# key = reshape_for_attention(key, batch_size, seq_len, self.num_heads, self.key_dim)\n",
    "# value = reshape_for_attention(value, batch_size, seq_len, self.num_heads, self.key_dim)\n",
    "\n",
    "# # Transpose to (batch_size, num_heads, seq_len, key_dim)\n",
    "# query = tf.transpose(query, [0, 2, 1, 3])\n",
    "# key = tf.transpose(key, [0, 2, 1, 3])\n",
    "# value = tf.transpose(value, [0, 2, 1, 3])\n",
    "\n",
    "# # Calculate attention scores\n",
    "# scale = tf.cast(self.key_dim, tf.float32) ** -0.5\n",
    "# attention_scores = tf.matmul(query, key, transpose_b=True) * scale\n",
    "\n",
    "# # Apply softmax\n",
    "# attention_weights = tf.nn.softmax(attention_scores)\n",
    "\n",
    "# # Apply dropout\n",
    "# if self.dropout_rate > 0:\n",
    "#     attention_weights = tf.nn.dropout(attention_weights, self.dropout_rate)\n",
    "\n",
    "# # Apply attention to values\n",
    "# attended_values = tf.matmul(attention_weights, value)\n",
    "\n",
    "# # Reshape back\n",
    "# attended_values = tf.transpose(attended_values, [0, 2, 1, 3])\n",
    "# attended_values = tf.reshape(attended_values, [batch_size, seq_len, self.num_heads * self.key_dim])\n",
    "\n",
    "# # Final linear transformation\n",
    "# outputs = self.combine_heads(attended_values)\n",
    "\n",
    "# return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "987add8d-de43-4173-b702-75096e83c6ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Custom attention layer compatible with TFLite\n",
    "class TFLiteCompatibleAttention(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, num_heads, key_dim, dropout=0.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.key_dim = key_dim\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "        # Initialize dense layers for Q, K, V\n",
    "        self.query_dense = tf.keras.layers.Dense(num_heads * key_dim)\n",
    "        self.key_dense = tf.keras.layers.Dense(num_heads * key_dim)\n",
    "        self.value_dense = tf.keras.layers.Dense(num_heads * key_dim)\n",
    "        self.combine_heads = tf.keras.layers.Dense(key_dim * num_heads)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Reshape inputs\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        seq_len = tf.shape(inputs)[1]\n",
    "\n",
    "        # Linear transformations\n",
    "        query = self.query_dense(inputs)\n",
    "        key = self.key_dense(inputs)\n",
    "        value = self.value_dense(inputs)\n",
    "\n",
    "        # Reshape to (batch_size, seq_len, num_heads, key_dim)\n",
    "        def reshape_for_attention(x, batch_size, seq_len, num_heads, key_dim):\n",
    "            return tf.reshape(x, [batch_size, seq_len, num_heads, key_dim])\n",
    "\n",
    "        query = reshape_for_attention(query, batch_size, seq_len, self.num_heads, self.key_dim)\n",
    "        key = reshape_for_attention(key, batch_size, seq_len, self.num_heads, self.key_dim)\n",
    "        value = reshape_for_attention(value, batch_size, seq_len, self.num_heads, self.key_dim)\n",
    "\n",
    "        # Transpose to (batch_size, num_heads, seq_len, key_dim)\n",
    "        query = tf.transpose(query, [0, 2, 1, 3])\n",
    "        key = tf.transpose(key, [0, 2, 1, 3])\n",
    "        value = tf.transpose(value, [0, 2, 1, 3])\n",
    "\n",
    "        # Calculate attention scores\n",
    "        scale = tf.cast(self.key_dim, tf.float32) ** -0.5\n",
    "        attention_scores = tf.matmul(query, key, transpose_b=True) * scale\n",
    "\n",
    "        # Apply softmax\n",
    "        attention_weights = tf.nn.softmax(attention_scores)\n",
    "\n",
    "        # Apply dropout\n",
    "        if self.dropout_rate > 0:\n",
    "            attention_weights = tf.nn.dropout(attention_weights, self.dropout_rate)\n",
    "\n",
    "        # Apply attention to values\n",
    "        attended_values = tf.matmul(attention_weights, value)\n",
    "\n",
    "        # Reshape back\n",
    "        attended_values = tf.transpose(attended_values, [0, 2, 1, 3])\n",
    "        attended_values = tf.reshape(attended_values, [batch_size, seq_len, self.num_heads * self.key_dim])\n",
    "\n",
    "        # Final linear transformation\n",
    "        outputs = self.combine_heads(attended_values)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "870efa48-ce2e-42d6-9f62-a9ced375a02d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, Dropout, LayerNormalization, GlobalAveragePooling1D,\n",
    "    Reshape, Add, Activation\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\"\"\"\n",
    "Build model using TFLite compatible attention\n",
    "\"\"\"\n",
    "def build_tflite_compatible_transformer_model(seq_length, num_features, pred_length, num_classes):\n",
    "\n",
    "    print(\"debug: build_tflite_compatible_model2...\")\n",
    "    inputs = Input(shape=(seq_length, num_features))\n",
    "    print(seq_length, num_features)\n",
    "\n",
    "    # Initial processing\n",
    "    x = Dense(128, activation='relu')(inputs)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "\n",
    "    # Custom attention blocks\n",
    "    for _ in range(4):\n",
    "        # Attention\n",
    "        attention_output = TFLiteCompatibleAttention(\n",
    "            num_heads=4, \n",
    "            key_dim=32,  # Do not change this\n",
    "            dropout=0.1\n",
    "        )(x)\n",
    "        x = Add()([x, attention_output])\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "        # FFN\n",
    "        ffn = Dense(256, activation='relu')(x)\n",
    "        ffn = Dropout(0.1)(ffn)\n",
    "        ffn = Dense(128)(ffn)\n",
    "        x = Add()([x, ffn])\n",
    "        x = LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "    # Output processing\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(pred_length * num_classes)(x)\n",
    "    x = Reshape((pred_length, num_classes))(x)\n",
    "    outputs = Activation('softmax')(x)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72a67548-cb6f-49e0-b346-22dd771fa30c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug: Adding rolling features...\n",
      "Adding rolling features for services...\n",
      "Calculating usage ratios...\n",
      "Rolling features added for all services.\n",
      "debug: Preparing sequences...\n",
      "Training shapes: X=(78, 672, 44), y=(78, 672, 5)\n",
      "debug: Building and compiling transformer model...\n",
      "debug: build_tflite_compatible_model2...\n",
      "672 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 15:38:13.015470: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2024-12-04 15:38:13.015495: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2024-12-04 15:38:13.015504: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2024-12-04 15:38:13.015536: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2024-12-04 15:38:13.015549: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug: Adding rolling features...\n",
      "debug: Preparing sequences...\n",
      "Training shapes: X=(78, 672, 44), y=(78, 672, 5)\n",
      "debug: Building and compiling transformer model...\n",
      "debug: build_tflite_compatible_model2...\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "# Add rolling features\n",
    "print(\"debug: Adding rolling features...\")\n",
    "df_pivot.head(5)\n",
    "df_pivot = add_rolling_features(df_pivot)\n",
    "df_pivot.head(100)\n",
    "\n",
    "# Prepare sequences\n",
    "print(\"debug: Preparing sequences...\")\n",
    "X, y, label_encoder, scaler, features = prepare_sequences(df_pivot)\n",
    "\n",
    "# Split data\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"Training shapes: X={X_train.shape}, y={y_train.shape}\")\n",
    "\n",
    "# Build and compile\n",
    "print(\"debug: Building and compiling transformer model...\")\n",
    "\n",
    "\n",
    "model = build_tflite_compatible_transformer_model(\n",
    "    seq_length=X_train.shape[1],\n",
    "    num_features=X_train.shape[2],\n",
    "    pred_length=y_train.shape[1],\n",
    "    num_classes=y_train.shape[2]\n",
    ")\n",
    "\n",
    "# Save initial shapes and parameters\n",
    "model_config = {\n",
    "    'seq_length': X_train.shape[1],\n",
    "    'num_features': X_train.shape[2],\n",
    "    'pred_length': y_train.shape[1],\n",
    "    'num_classes': y_train.shape[2],\n",
    "    'feature_columns': features\n",
    "}\n",
    "\n",
    "joblib.dump(model_config, 'model_config.pkl')\n",
    "\n",
    "# Compile with warmup\n",
    "initial_learning_rate = 0.001\n",
    "\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=1000,\n",
    "    alpha=0.0001\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"debug: Adding rolling features...\")\n",
    "print(\"debug: Preparing sequences...\")\n",
    "print(f\"Training shapes: X={X_train.shape}, y={y_train.shape}\")\n",
    "print(\"debug: Building and compiling transformer model...\")\n",
    "print(\"debug: build_tflite_compatible_model2...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30099ede-3a29-4bea-809b-2864948de146",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# print(\"Num GPUs Available:\", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85b9a161-a1af-4373-9994-7244d3c223c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# # Create a simple tensor operation\n",
    "# a = tf.constant([[1.0, 2.0, 3.0]])\n",
    "# b = tf.constant([[4.0], [5.0], [6.0]])\n",
    "# c = tf.matmul(a, b)\n",
    "\n",
    "# print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0aa19d29-aba0-4dd6-90d2-fa63dcbd21f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress INFO and WARNING logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81153ec-ec04-400b-8f18-b81d3c066d75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug: Training model...\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 15:38:23.288284: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "print(\"debug: Training model...\")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_accuracy',\n",
    "            patience=10,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_accuracy',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=0.0001\n",
    "        )\n",
    "    ],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e4709b-495b-47d5-8b2b-26c88b583486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation loss\n",
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.plot(history.history['loss'], label='Training Loss (MSE)')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss (MSE)')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot\n",
    "plot_training_history(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aec693a-17dc-4b67-858e-2ebc842c9af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('qos', exist_ok=True)\n",
    "print(\"\\n Savings model...\")\n",
    "model.save(\"./qos/qos_predicted_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d50e80-cc20-4290-8cc1-66d44a51b380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "# Generate predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert predictions and true labels to class indices\n",
    "y_pred_classes = np.argmax(y_pred, axis=-1)  # Predicted classes\n",
    "y_true_classes = np.argmax(y_test, axis=-1)  # True classes\n",
    "\n",
    "# Calculate per-group accuracy\n",
    "print(\"\\nPer-Group Accuracy:\")\n",
    "for group in label_encoder.classes_:\n",
    "    group_idx = label_encoder.transform([group])[0]  # Index for the group\n",
    "    mask = y_true_classes == group_idx  # Mask for the current group\n",
    "    group_acc = np.mean(y_pred_classes[mask] == y_true_classes[mask])  # Group accuracy\n",
    "    print(f\"{group}: {group_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80651a6-3cb1-4e4e-a08b-267677a878aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fd3e24-17b2-416d-9167-c099daf8d5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c771ce57-a9c5-4d2d-8b44-cd8514149441",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
